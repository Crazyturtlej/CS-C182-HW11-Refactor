{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3cd3e2f3",
      "metadata": {
        "id": "3cd3e2f3"
      },
      "source": [
        "# Scaling Laws\n",
        "\n",
        "A common question in neural network training is, how should I select my hyperparameters? While a proper hyperparameter sweep will always provide the best answer, sweeps can become impractical especially at larger network sizes. In this case, the field has converged to two main options: 1), copy what a previous project used (which is always a good starting point), or 2) derive *scaling laws* which can predict what the best hyperparameters will be.\n",
        "\n",
        "In this homework question, we will derive a simple scaling law for the optimal learning rate under varying batch sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf87aa21",
      "metadata": {
        "id": "bf87aa21"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46553266",
      "metadata": {
        "id": "46553266"
      },
      "source": [
        "First, let's consider the case of a simple least-squares gradient descent problem. We will define our dataset using a randomly-sampled ground truth linear mapping, and our training samples will be augmented by some amount of noise. For this homework, we will focus on the question of **how should learning rate scale with batch size?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61fe981",
      "metadata": {
        "id": "d61fe981"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "N = 10000  # number of data samples\n",
        "D = 16  # input dimension\n",
        "sigma = 5.0  # noise std\n",
        "\n",
        "X_train = np.random.randn(N, D)  # gaussian inputs for training data\n",
        "X_test = np.random.randn(1_000, D)  # gaussian inputs for test data\n",
        "w_true = np.random.randn(D, D)  # ground truth weight\n",
        "y_train = X_train @ w_true + sigma * np.random.randn(N, D)  # noisy outputs training data\n",
        "y_test = X_test @ w_true + sigma * np.random.randn(1_000, D)  # noisy outputs test data\n",
        "\n",
        "\n",
        "def analytical_solution(X: np.ndarray, y: np.ndarray, n: int) -> np.ndarray:\n",
        "    \"\"\"Calculates the analytical solution for a least-squares problem.\n",
        "\n",
        "    Args:\n",
        "        X: Input data matrix.\n",
        "        y: Target data matrix.\n",
        "        n: Number of samples to consider.\n",
        "\n",
        "    Returns:\n",
        "        The analytical weight matrix.\n",
        "    \"\"\"\n",
        "    pseudo_inv_X = np.linalg.pinv(X[:n, :], rcond=0)  # pseudo-inverse of X\n",
        "    w_analytical = pseudo_inv_X @ y[:n, :]  # analytical solution\n",
        "    return w_analytical\n",
        "\n",
        "\n",
        "def compute_mse(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
        "    \"\"\"Computes the Mean Squared Error (MSE).\n",
        "\n",
        "    Args:\n",
        "        X: Input data matrix.\n",
        "        y: True target values.\n",
        "        w: Predicted weights.\n",
        "\n",
        "    Returns:\n",
        "        The Mean Squared Error.\n",
        "    \"\"\"\n",
        "    y_pred = X @ w  # predicted outputs\n",
        "    mse = np.mean((y - y_pred)**2)  # mean squared error\n",
        "    return mse\n",
        "\n",
        "\n",
        "def compute_losses(\n",
        "    X_train: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    X_test: np.ndarray,\n",
        "    y_test: np.ndarray,\n",
        "    N: int\n",
        ") -> tuple[list[int], list[float], list[float]]:\n",
        "    \"\"\"Computes training and testing losses for analytical solutions across different sample sizes.\n",
        "\n",
        "    Args:\n",
        "        X_train: Training input data.\n",
        "        y_train: Training target data.\n",
        "        X_test: Test input data.\n",
        "        y_test: Test target data.\n",
        "        N: Total number of samples.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing lists of sample sizes, training losses, and test losses.\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    ns = list(range(1, N + 1))\n",
        "    for n in ns:\n",
        "        w_analytical = analytical_solution(X_train, y_train, n)\n",
        "        train_mse = compute_mse(X_train, y_train, w_analytical)\n",
        "        test_mse = compute_mse(X_test, y_test, w_analytical)\n",
        "        train_losses.append(train_mse)\n",
        "        test_losses.append(test_mse)\n",
        "    return ns, train_losses, test_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ead2daf0",
      "metadata": {
        "id": "ead2daf0"
      },
      "outputs": [],
      "source": [
        "def run_sgd(iters: int, batch_size: int, lr: float) -> tuple[np.ndarray, list[float], list[float]]:\n",
        "    \"\"\"Runs Stochastic Gradient Descent for a least-squares problem.\n",
        "\n",
        "    Args:\n",
        "        iters: Number of training iterations.\n",
        "        batch_size: Size of each mini-batch.\n",
        "        lr: Learning rate.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the final weights, training MSEs, and test MSEs.\n",
        "    \"\"\"\n",
        "    np.random.seed(0)\n",
        "    w_init = np.random.randn(D, D)\n",
        "\n",
        "    test_sgd_mse = []\n",
        "    train_sgd_mse = []\n",
        "\n",
        "    w = w_init\n",
        "    for i in range(iters):\n",
        "        idx = np.random.randint(0, N, batch_size)\n",
        "        x = X_train[idx]\n",
        "        y = y_train[idx]\n",
        "        # Gradient for least squares: d/dw (1/N * ||Xw - y||^2) = (2/N) * X.T @ (Xw - y)\n",
        "        grad = (2 / batch_size) * (x.T @ x @ w - x.T @ y)  # shape (D, D)\n",
        "\n",
        "        w = w - lr * grad\n",
        "        train_mse = compute_mse(x, y, w)\n",
        "        test_mse = compute_mse(X_test, y_test, w)\n",
        "        test_sgd_mse.append(test_mse)\n",
        "        train_sgd_mse.append(train_mse)\n",
        "\n",
        "    return w, train_sgd_mse, test_sgd_mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30317c8c",
      "metadata": {
        "id": "30317c8c"
      },
      "outputs": [],
      "source": [
        "w, train_sgd_mse, test_sgd_mse = run_sgd(iters=100, batch_size=32, lr=0.01)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(train_sgd_mse, label='Train Loss')\n",
        "ax.plot(test_sgd_mse, label='Test Loss')\n",
        "ax.set_xlabel('Number of Training Iterations')\n",
        "ax.set_ylabel('Squared Error')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6fb618d",
      "metadata": {
        "id": "a6fb618d"
      },
      "source": [
        "We can plot the above curves on log-linear scale while subtracting off the irreducible error of $\\sigma^2$ to see the linear decay of the squared error. This allows for a better view of the convergence rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65989481",
      "metadata": {
        "id": "65989481"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "train_sgd_mse_res = [mse - sigma**2 for mse in train_sgd_mse]\n",
        "test_sgd_mse_res = [mse - sigma**2 for mse in test_sgd_mse]\n",
        "ax.plot(train_sgd_mse_res, label='Train Loss')\n",
        "ax.plot(test_sgd_mse_res, label='Test Loss')\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel('Number of Training Iterations')\n",
        "ax.set_ylabel(r'Squared Error - $\\sigma^2$')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df9a7f29",
      "metadata": {
        "id": "df9a7f29"
      },
      "source": [
        "## Q1: Scaling law for least-squares SGD.\n",
        "\n",
        "Perform a sweep over learning rates. Consider the batch sizes between [2, 4, 16, 64, 256, 1024, 2048], and sweep over learning rates logarithmically with a resolution of $1.25$. For example, you should consider the learning rate of $0.001, 0.001 * 1.25, 0.001 * 1.25^2$, etc. **Make sure your learning rate sweep covers the optimal LR for each batch size (e.g. your optimal LR should not be at the boundary of your learning rate intervals.) You should be able to sweep around ~32 learning rates per batch size.\n",
        "\n",
        "- Plot your learning rates on the same graph, with each batchsize as a different curve. You curve should resemble the example provided.\n",
        "- Plot the relationship between batch size (x-axis) and the optimal learning rate (y-axis). What function does this resemble?\n",
        "\n",
        "Hint: Many runs will result in very high losses if the iteration diverges. It will help to clip the losses to some ceiling value before plotting.\n",
        "\n",
        "Hint 2: You may find that some batch sizes have a wide basin of optimal learning rates that perform roughly equivalently. In this case, it may help to plot the *average* learning rate that is within X% of the optimal. This can make the relationship more clear.\n",
        "\n",
        "![fig-example](https://github.com/Berkeley-CS182/cs182fa25_public/blob/main/hw11/code/fig-example.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28df4291",
      "metadata": {
        "id": "28df4291"
      },
      "outputs": [],
      "source": [
        "#### TODO:\n",
        "\n",
        "\n",
        "####\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45dea9e0",
      "metadata": {
        "id": "45dea9e0"
      },
      "source": [
        "## Q2: MLP with SGD\n",
        "\n",
        "We will now repeat a similar study, using a two-layer MLP rather than a linear regression. Again, conduct a sweep on the relationship between batch size and optimal learning rate. How does this relationship compare to the optimum for least-squares SGD?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5d52a04",
      "metadata": {
        "id": "f5d52a04"
      },
      "outputs": [],
      "source": [
        "N = 10000  # number of data samples\n",
        "D = 4  # input dimension\n",
        "sigma = 2.0  # noise std\n",
        "\n",
        "np.random.seed(0)\n",
        "X_train = np.random.randn(N, D)  # gaussian inputs for training data\n",
        "X_test = np.random.randn(1_000, D)  # gaussian inputs for test data\n",
        "W1_true = np.random.randn(D, D)  # ground truth weight\n",
        "W2_true = np.random.randn(D, D)  # ground truth weight\n",
        "y_train = np.maximum(0, X_train @ W1_true) @ W2_true + sigma * np.random.randn(N, D)  # noisy outputs training data\n",
        "y_test = np.maximum(0, X_test @ W1_true) @ W2_true + sigma * np.random.randn(1_000, D)  # noisy outputs test data\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "\n",
        "def model_inst(input_dim: int) -> tuple[torch.nn.Module, torch.nn.modules.loss._Loss]:\n",
        "    \"\"\"Initializes a two-layer MLP model and MSE loss function.\n",
        "\n",
        "    Args:\n",
        "        input_dim: The input and output dimension of the MLP.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the MLP model and the MSE loss function.\n",
        "    \"\"\"\n",
        "    model = torch.nn.Sequential(\n",
        "        torch.nn.Linear(input_dim, input_dim),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(input_dim, input_dim)\n",
        "    )\n",
        "\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    return model, loss_fn\n",
        "\n",
        "\n",
        "def train_mlp_sgd(iters: int, batch_size: int, lr: float, opt: str = 'SGD') -> tuple[list[float], list[float]]:\n",
        "    \"\"\"Trains an MLP model using SGD or Adam optimizer.\n",
        "\n",
        "    Args:\n",
        "        iters: Number of training iterations.\n",
        "        batch_size: Size of each mini-batch.\n",
        "        lr: Learning rate.\n",
        "        opt: Optimizer to use ('SGD' or 'Adam').\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing lists of training MSEs and test MSEs.\n",
        "    \"\"\"\n",
        "    torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "    model, loss_fn = model_inst(D)\n",
        "    scheduler = None\n",
        "\n",
        "    if opt == 'SGD':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    elif opt == 'Adam':\n",
        "        # TODO\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.5), weight_decay=0.01)\n",
        "        scheduler1 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.001, end_factor=1.0, total_iters=10)\n",
        "        scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=iters - 10)\n",
        "        scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
        "            optimizer,\n",
        "            schedulers=[scheduler1, scheduler2],\n",
        "            milestones=[10],\n",
        "        )\n",
        "        #\n",
        "    else:\n",
        "        raise ValueError(\"Optimizer not recognized\")\n",
        "\n",
        "    test_sgd_mse = []\n",
        "    train_sgd_mse = []\n",
        "\n",
        "    for i in range(iters):\n",
        "        idx = np.random.randint(0, N, batch_size)\n",
        "        x = torch.tensor(X_train[idx], dtype=torch.float32)\n",
        "        y = torch.tensor(y_train[idx], dtype=torch.float32)\n",
        "\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        test_sgd_mse.append(loss_fn(model(X_test_tensor), y_test_tensor).item())\n",
        "        train_sgd_mse.append(loss_fn(model(x), y).item())\n",
        "\n",
        "    return train_sgd_mse, test_sgd_mse\n",
        "\n",
        "\n",
        "mlp_train_mse, mlp_test_mse = train_mlp_sgd(iters=100, batch_size=256, lr=0.1)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(mlp_train_mse, label='Train Loss')\n",
        "ax.plot(mlp_test_mse, label='Test Loss')\n",
        "ax.set_xlabel('Number of Training Iterations')\n",
        "ax.set_ylabel('Squared Error')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db50e827",
      "metadata": {
        "id": "db50e827"
      },
      "source": [
        "Again, let's plot on log-linear scale minus the offset. Notice that the MLP converges to $\\approx \\sigma^2 + 0.57$ as opposed to $\\sigma^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af588a90",
      "metadata": {
        "id": "af588a90"
      },
      "outputs": [],
      "source": [
        "eps = 0.57\n",
        "mlp_train_mse_res = [mse - (sigma**2 + eps) for mse in mlp_train_mse]\n",
        "mlp_test_mse_res = [mse - (sigma**2 + eps) for mse in mlp_test_mse]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(mlp_train_mse_res, label='Train Loss')\n",
        "ax.plot(mlp_test_mse_res, label='Test Loss')\n",
        "ax.set_xlabel('Number of Training Iterations')\n",
        "ax.set_ylabel(r'Squared Error - $\\sigma^2$')\n",
        "ax.set_yscale('log')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcc13f8a",
      "metadata": {
        "id": "fcc13f8a"
      },
      "outputs": [],
      "source": [
        "###### TODO\n",
        "\n",
        "\n",
        "######"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b0c0ea3",
      "metadata": {
        "id": "5b0c0ea3"
      },
      "source": [
        "#### Q3: MLP with Adam.\n",
        "\n",
        "Finally, we will repeat the scaling law curve, but using the Adam optimizer. This time, implement a learning rate schedule, such that the learning rate has a linear warmup for 10 steps, then uses cosine decay for the rest of training. Use a beta1=0.5, beta2=0.5, and a weight decay of 0.001. As before, plot the comparison curves, then plot a curve of the batch size vs. optimal learning rate. Does the scaling for learning rate change with Adam vs. SGD?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f3363e8",
      "metadata": {
        "id": "6f3363e8"
      },
      "outputs": [],
      "source": [
        "##### TODO\n",
        "\n",
        "\n",
        "\n",
        "######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e28985d9",
      "metadata": {
        "id": "e28985d9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "frog",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}